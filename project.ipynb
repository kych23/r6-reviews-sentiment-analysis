{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project guidelines\n",
    "\n",
    "**Note:** Use these guidelines if and only if you are pursuing a **final project of your own design**. For those taking the final exam instead of the project, see the (separate) final exam notebook.\n",
    "\n",
    "### The task\n",
    "\n",
    "Your task is to: identify an interesting problem connected to the humanities or humanistic social sciences that's addressable with the help of computational methods, formulate a hypothesis about it, devise an experiment or experiments to test your hypothesis, present the results of your investigations, and discuss your findings.\n",
    "\n",
    "These tasks essentially replicate the process of writing an academic paper. You can think of your project as a paper in miniature.\n",
    "\n",
    "You are free to present each of these tasks as you see fit. You should use narrative text (that is, your own writing in a markdown cell), citations of others' work, numerical results, tables of data, and static and/or interactive visualizations as appropriate. Total length is flexible and depends on the number of people involved in the work, as well as the specific balance you strike between the ambition of your question and the sophistication of your methods. But be aware that numbers never, ever speak for themselves. Quantitative results presented without substantial discussion will not earn high marks. \n",
    "\n",
    "Your project should reflect, at minimum, ten **or more** hours of work by each participant, though you will be graded on the quality of your work, not the amount of time it took you to produce it. Most high-quality projects represent twenty or more hours of work by each member.\n",
    "\n",
    "#### Pick an important and interesting problem!\n",
    "\n",
    "No amount of technical sophistication will overcome a fundamentally uninteresting problem at the core of your work. You have seen many pieces of successful computational humanities research over the course of the semester. You might use these as a guide to the kinds of problems that interest scholars in a range of humanities disciplines. You may also want to spend some time in the library, reading recent books and articles in the professional literature. **Problem selection and motivation are integral parts of the project.** Do not neglect them.\n",
    "\n",
    "### Format\n",
    "\n",
    "You should submit your project as a PDF document created using the included $\\LaTeX{}$ template. Consult the template for information on formatting and what is expected in each section. You can use your favorite text editor or something like [Overleaf](https://www.overleaf.com/) to edit this document. You will also submit this Jupyter notebook, along with all data necessary to reproduce your analysis. If your dataset is too large to share easily, let us know in advance so that we can find a workaround. \n",
    "\n",
    "All code used in the project should be present in the notebook (except for widely-available libraries that you import), but **be sure that we can read and understand your report in full without rerunning the code**. \n",
    "\n",
    "Because you are submitting essentially a mini-paper in the PDF writeup, I don't have any particular formatting expections for written material in this notebook. However, you should include **all code used when completing the final project, with comments added for clarity**. It should be straightforward to map code from the notebook to sections/figures/results in your paper, and vice versa.\n",
    "\n",
    "### Grading\n",
    "\n",
    "This project takes the place of the take-home final exam for the course. It is worth 35% of your overall grade. You will be graded on the quality and ambition of each aspect of the project. No single component is more important than the others.\n",
    "\n",
    "### Practical details\n",
    "\n",
    "* The project is due at **4:30 PM EST on Wednesday, December 17** via upload to CMS of a single zip file containing your fully executed Jupyter notebook report, a PDF copy of the notebook, and all associated data. **You may not use slip days for the final project or exam**. \n",
    "* You may work alone or in a group of up to three total members.\n",
    "    * If you work in a group, be sure to list the names of the group members.\n",
    "    * For groups, create your group on CMS and submit one notebook for the entire group. **Each group should also submit a statement of responsibility** that describes in general terms who performed which parts of the project.\n",
    "* You may post questions on Ed, but should do so privately (visible to course staff only).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your info\n",
    "* NetID(s): kgc42\n",
    "* Name(s): Kyle Chu\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERVIEW\n",
    "dataset: https://www.kaggle.com/datasets/najzeko/steam-reviews-2021\n",
    "\n",
    "problem: Among Steam reviews whose text is negative in sentiment, what linguistic patterns distinguish reviews that recommend the game from those that do not recommend it?\n",
    "\n",
    "hypothesis: Within negative-sentiment reviews, those that still recommend the game focus more on core gameplay quality and fun, whereas negative reviews that do not recommend focus more on bugs, performance problems, and monetization/price complaints.\n",
    "\n",
    "data cleaning\n",
    "- only take reviews in English and game_id = 730 (csgo)\n",
    "- balance the number of recommended vs not recommended reviews \n",
    "- remove really short reviews \n",
    "- use a prebuilt sentiment model or hand label reviews to train my own classifier, then classify the reviews and only keep the negative ones\n",
    "\n",
    "data processing\n",
    "- vectorize reviews based on tf-idf or n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROMPT\n",
    "\n",
    "You are an expert data scientist and ML engineer working in a Jupyter/Python environment with access to the Kaggle dataset:\n",
    "\n",
    "  \"Steam Reviews 2021\" → https://www.kaggle.com/datasets/najzeko/steam-reviews-2021\n",
    "\n",
    "Your task is to implement a complete, reproducible pipeline (ideally as a single, well-structured Jupyter notebook or a main .py file plus helpers) to test the following hypothesis (H1) using this dataset.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "PROJECT OVERVIEW\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Research question:\n",
    "  Among Steam reviews whose text is negative in sentiment, what linguistic patterns distinguish reviews that recommend the game from those that do not recommend it?\n",
    "\n",
    "Hypothesis H1 (content-focused):\n",
    "  Within negative-sentiment reviews, those that still recommend the game focus more on core gameplay quality and fun, whereas negative reviews that do not recommend the game focus more on bugs/performance problems and monetization/price complaints.\n",
    "\n",
    "We operationalize this as:\n",
    "  • Outcome Y: whether the review recommends the game (steam’s \"voted_up\"/\"recommended\" flag).\n",
    "  • Condition: restrict to reviews whose text is negative according to a sentiment model.\n",
    "  • Predictors: complaint-type features derived from the text (lexicons and/or topic proportions), plus simple controls (e.g., review length, game fixed effects).\n",
    "\n",
    "You should produce:\n",
    "  1. Data loading / sampling code.\n",
    "  2. Sentiment analysis and definition of the \"negative subset\".\n",
    "  3. Text feature engineering (lexicon features + topic model).\n",
    "  4. Descriptive comparisons between \"negative+recommend\" vs \"negative+not recommend\".\n",
    "  5. A logistic regression model testing H1.\n",
    "  6. Basic robustness knobs (e.g., adjustable sentiment threshold).\n",
    "\n",
    "Write clean, modular, well-commented code that a student can read and modify.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "DATA HANDLING\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "1) Loading and sampling\n",
    "   • Assume the raw CSV is at a configurable path, e.g. DATA_PATH = \"steam_reviews.csv\".\n",
    "   • Use pandas to load the file, but because the dataset is large (GB-scale), implement one of:\n",
    "       – A random row sample (e.g., 500k–1M reviews) using `skiprows` / chunking, or\n",
    "       – Sampling per game (e.g., up to N reviews per game for the most-reviewed games).\n",
    "   • Expose sample size and sampling strategy as parameters at the top of the notebook/script.\n",
    "   • Keep at least the following columns (names may differ; infer from the header):\n",
    "       – review text (likely \"review\")\n",
    "       – \"voted_up\" (or \"recommended\") → boolean recommendation flag\n",
    "       – \"language\"\n",
    "       – game/app id (e.g., \"appid\")\n",
    "       – any other useful metadata (e.g., \"timestamp_created\", \"steam_purchase\", \"received_for_free\", etc., if available).\n",
    "\n",
    "2) Basic filtering\n",
    "   • Filter to English reviews: `language == 'english'` (or whatever exact label is in the CSV).\n",
    "   • Drop null/empty review texts.\n",
    "   • Drop very short reviews (e.g., fewer than 10 tokens).\n",
    "\n",
    "3) Provide a function:\n",
    "   `load_and_sample_data(path: str, sample_size: int, random_state: int) -> pd.DataFrame`\n",
    "   that returns a cleaned, sampled DataFrame ready for NLP.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "SENTIMENT ANALYSIS AND NEGATIVE SUBSET\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Goal: assign each review a sentiment score and define a “negative review” subset.\n",
    "\n",
    "1) Sentiment model\n",
    "   • Use VADER (from `nltk.sentiment.vader`) or another standard sentiment tool available in Python.\n",
    "   • Compute VADER's `compound` score in [-1, 1] for each review text.\n",
    "   • Add a column `sentiment_compound`.\n",
    "\n",
    "2) Define negativity\n",
    "   • Define a negativity threshold parameter, e.g.:\n",
    "       NEG_THRESHOLD = 0.0\n",
    "   • Create a binary indicator:\n",
    "       negative = (sentiment_compound < NEG_THRESHOLD)\n",
    "   • Filter the DataFrame to only rows where `negative` is True.\n",
    "   • In the code, make NEG_THRESHOLD easy to change (constant at top or function argument).\n",
    "\n",
    "3) Group labels for H1\n",
    "   • Inside the negative subset, define:\n",
    "       – Group A (love–hate): negative sentiment AND `voted_up == True`\n",
    "       – Group B (full rejection): negative sentiment AND `voted_up == False`\n",
    "   • Add a binary column `recommended_flag` = 1 if voted_up is true, else 0.\n",
    "   • Ensure you print basic counts:\n",
    "       – Number of negative reviews\n",
    "       – Counts of negative+recommended vs negative+not-recommended\n",
    "\n",
    "4) Encapsulate this logic in a function:\n",
    "   `add_sentiment_and_filter_negative(df: pd.DataFrame, neg_threshold: float) -> pd.DataFrame`\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "TEXT PREPROCESSING AND FEATURE ENGINEERING\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "We need features capturing complaint types: bugs/performance, monetization/price, and gameplay/fun/content.\n",
    "\n",
    "1) Text preprocessing\n",
    "   • Create a text-cleaning function that:\n",
    "       – lowercases\n",
    "       – removes URLs\n",
    "       – optionally removes punctuation except where needed for exclamation count\n",
    "       – tokenizes (use a simple tokenizer, it doesn’t need to be perfect)\n",
    "   • Store a cleaned text column (`clean_text`) for feature extraction.\n",
    "\n",
    "2) Lexicon-based features (for interpretability)\n",
    "   • In code, define three small lexicons (word lists) for:\n",
    "       – BUG/PERFORMANCE: [\"crash\", \"crashes\", \"lag\", \"laggy\", \"fps\", \"bug\", \"bugs\", \"glitch\", \"glitches\", \"freeze\", \"freezing\", \"stutter\", \"stuttering\", \"performance\", \"optimization\", \"optimized\", ...]\n",
    "       – MONETIZATION/PRICE: [\"microtransaction\", \"microtransactions\", \"mtx\", \"lootbox\", \"lootboxes\", \"dlc\", \"season pass\", \"pay2win\", \"pay-to-win\", \"cash grab\", \"overpriced\", \"refund\", \"sale\", \"expensive\", ...]\n",
    "       – GAMEPLAY/FUN/CONTENT: [\"fun\", \"gameplay\", \"combat\", \"balance\", \"content\", \"story\", \"graphics\", \"soundtrack\", \"music\", \"replay\", \"grind\", \"coop\", \"co-op\", \"friends\", ...]\n",
    "   • Implement a function that:\n",
    "       – Tokenizes each cleaned review,\n",
    "       – Counts occurrences from each lexicon,\n",
    "       – Normalizes per 100 tokens (e.g., (#lexicon_words / #tokens) * 100).\n",
    "   • Add numerical columns:\n",
    "       – bug_lex_per_100\n",
    "       – monet_lex_per_100\n",
    "       – gameplay_lex_per_100\n",
    "   • Additionally, compute simple stylistic features:\n",
    "       – review_len_tokens\n",
    "       – exclamation_count\n",
    "       – maybe profanity_count using a small predefined list of swear words.\n",
    "\n",
    "3) Topic modeling (optional but preferred)\n",
    "   • Use scikit-learn’s `CountVectorizer` + `LatentDirichletAllocation` OR BERTopic to fit a topic model on the negative subset.\n",
    "   • Choose a reasonable K (e.g., 10–20 topics, set as a parameter).\n",
    "   • After fitting:\n",
    "       – Print top words per topic to inspect.\n",
    "       – Produce a table or print-out with topic index and top 10–15 words.\n",
    "   • For each review, compute topic distribution (topic proportions) and add them as features:\n",
    "       – topic_0, topic_1, ..., topic_{K-1}\n",
    "   • These can be used in the regression and for descriptive comparisons.\n",
    "\n",
    "4) Structure this as functions:\n",
    "   • `build_lexicon_features(df: pd.DataFrame, lexicons: dict) -> pd.DataFrame`\n",
    "   • `fit_topic_model(clean_text_series: pd.Series, n_topics: int) -> (LDA_model, vectorizer, topic_proportions_df)`\n",
    "   • Make sure topic_proportions_df aligns with rows of the filtered DataFrame.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "DESCRIPTIVE ANALYSIS FOR H1\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Within the *negative* subset:\n",
    "\n",
    "1) Basic summaries\n",
    "   • Print counts and proportions:\n",
    "       – negative & recommended_flag == 1\n",
    "       – negative & recommended_flag == 0\n",
    "\n",
    "2) Lexicon features by group\n",
    "   • Compute mean and standard deviation of:\n",
    "       – bug_lex_per_100\n",
    "       – monet_lex_per_100\n",
    "       – gameplay_lex_per_100\n",
    "     separately for recommended_flag == 1 vs 0.\n",
    "   • Perform significance tests (e.g., scipy.stats t-test or Mann–Whitney U) on differences in means for each feature.\n",
    "   • Create a small summary table of these statistics and p-values.\n",
    "\n",
    "3) If topic modeling is used:\n",
    "   • Compute average topic proportions per group (recommended_flag == 1 vs 0).\n",
    "   • Visualize selected topics with barplots of mean topic proportion by group.\n",
    "\n",
    "4) Plotting\n",
    "   • Use matplotlib or seaborn to:\n",
    "       – Plot distributions (e.g., boxplots or violin plots) of bug_lex_per_100, monet_lex_per_100, gameplay_lex_per_100 by recommended_flag.\n",
    "       – Optionally, bar charts for topic mean differences.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "LOGISTIC REGRESSION FOR H1\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "We want a multivariate model where the outcome is recommended_flag inside the negative subset, predicted by complaint-type features.\n",
    "\n",
    "1) Model specification\n",
    "   • Use statsmodels (preferred) or scikit-learn; if possible, use statsmodels for interpretable coefficients and p-values.\n",
    "   • Outcome:\n",
    "       – Y_i = recommended_flag (1 = recommended, 0 = not recommended)\n",
    "   • Predictors:\n",
    "       – bug_lex_per_100\n",
    "       – monet_lex_per_100\n",
    "       – gameplay_lex_per_100\n",
    "       – review_len_tokens\n",
    "       – (optionally) selected topic proportions (e.g., 3–5 topics that clearly correspond to gameplay/bugs/price)\n",
    "       – (optionally) game fixed effects (dummy variables for top N games, or game ID as a random effect if you prefer).\n",
    "\n",
    "2) Fit the model\n",
    "   • Standardize/scale predictors if needed.\n",
    "   • Fit a logistic regression:\n",
    "       – For statsmodels: use `Logit` or `GLM(family=Binomial())`.\n",
    "       – Cluster standard errors by game/app id if straightforward, or at least report robust standard errors.\n",
    "   • Print the full model summary (coefficients, standard errors, z-statistics, p-values).\n",
    "\n",
    "3) Hypothesis tests for H1\n",
    "   • Focus on signs and significance of complaint-type predictors:\n",
    "       – Expect bug_lex_per_100 and monet_lex_per_100 to have **negative** coefficients (more bug/price complaints → lower probability of recommendation among negative reviews).\n",
    "       – Expect gameplay_lex_per_100 to have a **positive** coefficient (more gameplay/fun talk → higher probability of recommendation among negative reviews).\n",
    "   • Optionally, compute marginal effects or predicted probabilities for:\n",
    "       – Low vs high gameplay_lex_per_100\n",
    "       – Low vs high bug_lex_per_100 / monet_lex_per_100\n",
    "\n",
    "4) Minimal predictive evaluation (optional)\n",
    "   • Split the negative subset into train/test, fit the logistic model (or an equivalent sklearn model) and report:\n",
    "       – Accuracy and ROC–AUC on the test set.\n",
    "   • This is secondary; emphasis is on inference and coefficient interpretation, not maximizing predictive performance.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "ROBUSTNESS KNOBS\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Implement at least the following as easy-to-change parameters:\n",
    "\n",
    "  • NEG_THRESHOLD: sentiment threshold for defining \"negative\" (e.g., 0.0, -0.1, -0.2).\n",
    "  • N_TOPICS: number of topics in the topic model.\n",
    "  • SAMPLE_SIZE: number of reviews to sample.\n",
    "\n",
    "Optionally, you can:\n",
    "  • Re-run the logistic regression under a more conservative negativity threshold (e.g., NEG_THRESHOLD = -0.2) and show that coefficients maintain their signs and remain reasonably similar in magnitude.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "CODE QUALITY AND ORGANIZATION\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Please:\n",
    "  • Structure code into clear sections with markdown headers if in a notebook:\n",
    "      – 0. Imports and constants\n",
    "      – 1. Load & sample data\n",
    "      – 2. Sentiment & negative subset\n",
    "      – 3. Text preprocessing & features\n",
    "      – 4. Descriptive analysis\n",
    "      – 5. Logistic regression for H1\n",
    "      – 6. (Optional) Robustness checks\n",
    "  • Write small helper functions instead of huge monolithic blocks.\n",
    "  • Add concise inline comments explaining:\n",
    "      – Why each step is done,\n",
    "      – How each feature relates back to H1.\n",
    "  • At the end, print a short textual summary (e.g., a few lines) interpreting the key coefficients and whether H1 is supported.\n",
    "\n",
    "Implement this entire pipeline now: create the necessary Python code (and markdown if using a notebook) so that, once the CSV path is set and nltk’s VADER is installed, the analysis for H1 can be run end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
