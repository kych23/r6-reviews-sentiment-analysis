{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final project guidelines\n",
        "\n",
        "**Note:** Use these guidelines if and only if you are pursuing a **final project of your own design**. For those taking the final exam instead of the project, see the (separate) final exam notebook.\n",
        "\n",
        "### The task\n",
        "\n",
        "Your task is to: identify an interesting problem connected to the humanities or humanistic social sciences that's addressable with the help of computational methods, formulate a hypothesis about it, devise an experiment or experiments to test your hypothesis, present the results of your investigations, and discuss your findings.\n",
        "\n",
        "These tasks essentially replicate the process of writing an academic paper. You can think of your project as a paper in miniature.\n",
        "\n",
        "You are free to present each of these tasks as you see fit. You should use narrative text (that is, your own writing in a markdown cell), citations of others' work, numerical results, tables of data, and static and/or interactive visualizations as appropriate. Total length is flexible and depends on the number of people involved in the work, as well as the specific balance you strike between the ambition of your question and the sophistication of your methods. But be aware that numbers never, ever speak for themselves. Quantitative results presented without substantial discussion will not earn high marks. \n",
        "\n",
        "Your project should reflect, at minimum, ten **or more** hours of work by each participant, though you will be graded on the quality of your work, not the amount of time it took you to produce it. Most high-quality projects represent twenty or more hours of work by each member.\n",
        "\n",
        "#### Pick an important and interesting problem!\n",
        "\n",
        "No amount of technical sophistication will overcome a fundamentally uninteresting problem at the core of your work. You have seen many pieces of successful computational humanities research over the course of the semester. You might use these as a guide to the kinds of problems that interest scholars in a range of humanities disciplines. You may also want to spend some time in the library, reading recent books and articles in the professional literature. **Problem selection and motivation are integral parts of the project.** Do not neglect them.\n",
        "\n",
        "### Format\n",
        "\n",
        "You should submit your project as a PDF document created using the included $\\LaTeX{}$ template. Consult the template for information on formatting and what is expected in each section. You can use your favorite text editor or something like [Overleaf](https://www.overleaf.com/) to edit this document. You will also submit this Jupyter notebook, along with all data necessary to reproduce your analysis. If your dataset is too large to share easily, let us know in advance so that we can find a workaround. \n",
        "\n",
        "All code used in the project should be present in the notebook (except for widely-available libraries that you import), but **be sure that we can read and understand your report in full without rerunning the code**. \n",
        "\n",
        "Because you are submitting essentially a mini-paper in the PDF writeup, I don't have any particular formatting expections for written material in this notebook. However, you should include **all code used when completing the final project, with comments added for clarity**. It should be straightforward to map code from the notebook to sections/figures/results in your paper, and vice versa.\n",
        "\n",
        "### Grading\n",
        "\n",
        "This project takes the place of the take-home final exam for the course. It is worth 35% of your overall grade. You will be graded on the quality and ambition of each aspect of the project. No single component is more important than the others.\n",
        "\n",
        "### Practical details\n",
        "\n",
        "* The project is due at **4:30 PM EST on Wednesday, December 17** via upload to CMS of a single zip file containing your fully executed Jupyter notebook report, a PDF copy of the notebook, and all associated data. **You may not use slip days for the final project or exam**. \n",
        "* You may work alone or in a group of up to three total members.\n",
        "    * If you work in a group, be sure to list the names of the group members.\n",
        "    * For groups, create your group on CMS and submit one notebook for the entire group. **Each group should also submit a statement of responsibility** that describes in general terms who performed which parts of the project.\n",
        "* You may post questions on Ed, but should do so privately (visible to course staff only).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your info\n",
        "* NetID(s): kgc42\n",
        "* Name(s): Kyle Chu\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import os\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import re\n",
        "\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Loading & Filtering\n",
        "\n",
        "We'll load the Steam Reviews dataset, filter to Rainbow Six Siege English reviews, and remove short reviews. Class balancing will be performed after sentiment analysis, once we have filtered to negative reviews only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /Users/kych2204/.cache/kagglehub/datasets/najzeko/steam-reviews-2021/versions/1\n"
          ]
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"najzeko/steam-reviews-2021\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "GAME_APPID = 359550"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from /Users/kych2204/.cache/kagglehub/datasets/najzeko/steam-reviews-2021/versions/1/steam_reviews.csv...\n"
          ]
        }
      ],
      "source": [
        "# load the CSV file\n",
        "\n",
        "csv_path = os.path.join(path, \"steam_reviews.csv\")\n",
        "print(f\"Loading data from {csv_path}...\")\n",
        "\n",
        "chunk_size = 50000\n",
        "chunks = []\n",
        "total_read = 0\n",
        "chunks_processed = 0\n",
        "\n",
        "for chunk in pd.read_csv(csv_path, chunksize=chunk_size, low_memory=False):\n",
        "    chunks_processed += 1\n",
        "    \n",
        "    chunk['app_id'] = pd.to_numeric(chunk['app_id'], errors='coerce')\n",
        "    \n",
        "    chunk_filtered = chunk[\n",
        "        (chunk['app_id'] == GAME_APPID) & \n",
        "        (chunk['language'].str.lower() == 'english')\n",
        "    ].copy()\n",
        "    \n",
        "    if len(chunk_filtered) > 0:\n",
        "        chunks.append(chunk_filtered)\n",
        "        total_read += len(chunk_filtered)\n",
        "    \n",
        "df = pd.concat(chunks, ignore_index=True)\n",
        "print(f\"\\nTotal Rainbow Six Siege English reviews loaded: {len(df):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean and filter the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['recommended'] = df['recommended'].astype(bool)\n",
        "\n",
        "# remove null/empty reviews\n",
        "initial_count = len(df)\n",
        "df = df[df['review'].notna()].copy()\n",
        "df = df[df['review'].str.strip() != ''].copy()\n",
        "print(f\"After removing null/empty reviews: {len(df):,} (removed {initial_count - len(df):,})\")\n",
        "\n",
        "# only keep reviews from users who purchased the game\n",
        "before_purchase_filter = len(df)\n",
        "df = df[df['steam_purchase'] == True].copy()\n",
        "print(f\"After filtering to steam_purchase == True: {len(df):,} (removed {before_purchase_filter - len(df):,})\")\n",
        "\n",
        "# remove very short reviews under 10 words\n",
        "df['token_count'] = df['review'].str.split().str.len()\n",
        "df = df[df['token_count'] >= 10].copy()\n",
        "print(f\"After removing reviews with < 10 tokens: {len(df):,}\")\n",
        "\n",
        "# check class distribution\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(df['recommended'].value_counts())\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# remove unneeded columns\n",
        "keep_cols = ['review', 'recommended', 'app_id', 'language', 'token_count', 'timestamp_created']\n",
        "\n",
        "df = df[keep_cols].copy()\n",
        "\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Sentiment Analysis & Negative Subset\n",
        "\n",
        "We'll use VADER sentiment analysis to identify negative reviews, then filter to only negative reviews for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    nltk.data.find('vader_lexicon')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon')\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration for sentiment analysis\n",
        "NEG_THRESHOLD = 0.0  # Reviews with compound score < this are considered negative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute sentiment scores for each review\n",
        "print(\"Computing sentiment scores...\")\n",
        "df['sentiment_compound'] = df['review'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
        "\n",
        "print(f\"Sentiment score statistics:\")\n",
        "print(f\"  Mean: {df['sentiment_compound'].mean():.3f}\")\n",
        "print(f\"  Median: {df['sentiment_compound'].median():.3f}\")\n",
        "print(f\"  Min: {df['sentiment_compound'].min():.3f}\")\n",
        "print(f\"  Max: {df['sentiment_compound'].max():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter to negative reviews only\n",
        "print(f\"\\nFiltering to negative reviews (sentiment_compound < {NEG_THRESHOLD})...\")\n",
        "df_negative = df[df['sentiment_compound'] < NEG_THRESHOLD].copy()\n",
        "\n",
        "print(f\"Total negative reviews: {len(df_negative):,}\")\n",
        "print(f\"Removed {len(df) - len(df_negative):,} non-negative reviews\")\n",
        "\n",
        "print(f\"\\nClass distribution in negative reviews:\")\n",
        "print(df_negative['recommended'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# balance datasets\n",
        "\n",
        "negative_recommended = df_negative[df_negative['recommended'] == True].copy()\n",
        "negative_not_recommended = df_negative[df_negative['recommended'] == False].copy()\n",
        "\n",
        "print(f\"\\nNegative + Recommended before balancing: {len(negative_recommended):,}\")\n",
        "print(f\"Negative + Not Recommended before balancing: {len(negative_not_recommended):,}\")\n",
        "\n",
        "n_sample = min(len(negative_recommended), len(negative_not_recommended))\n",
        "print(f\"\\nSampling {n_sample:,} reviews from each class...\")\n",
        "\n",
        "negative_rec_sample = negative_recommended.sample(n=n_sample, random_state=42)\n",
        "negative_not_rec_sample = negative_not_recommended.sample(n=n_sample, random_state=42)\n",
        "\n",
        "df_negative_balanced = pd.concat([negative_rec_sample, negative_not_rec_sample], ignore_index=True)\n",
        "df_negative_balanced = df_negative_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nFinal balanced negative dataset: {len(df_negative_balanced):,} reviews\")\n",
        "print(f\"Class distribution after balancing:\")\n",
        "print(df_negative_balanced['recommended'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# turn recommended column into binary\n",
        "df_negative_balanced['recommended'] = df_negative_balanced['recommended'].astype(int)\n",
        "\n",
        "df = df_negative_balanced.copy()\n",
        "\n",
        "print(f\"\\nFinal dataset for analysis: {len(df):,} negative reviews\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Text Preprocessing & Feature Engineering\n",
        "\n",
        "We'll preprocess the text, create lexicon-based features for bugs/performance, monetization/price, and gameplay/fun, and fit a topic model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    '''Clean text for feature extraction'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['review'].apply(preprocess_text)\n",
        "print(f\"Preprocessed {len(df):,} reviews\")\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha and not token.is_space]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "df['clean_text'] = df['clean_text'].apply(lemmatize_text)\n",
        "print(f\"Lemmatized {len(df):,} reviews for topic modeling\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lexicon-Based Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bug / Performance complaints\n",
        "QUALITY_LEXICON = [\n",
        "    \"crash\",\n",
        "    \"lag\", \"laggy\",\n",
        "    \"fps\",\n",
        "    \"bug\",\n",
        "    \"glitch\",\n",
        "    \"freeze\",\n",
        "    \"stutter\",\n",
        "    \"performance\",\n",
        "    \"optimization\",\n",
        "    \"optimize\",\n",
        "    \"broken\",\n",
        "    \"unplayable\",\n",
        "    \"frame\",\n",
        "    \"server\",\n",
        "    \"disconnect\",\n",
        "    \"ping\",\n",
        "    \"latency\",\n",
        "    \"drop\",\n",
        "]\n",
        "\n",
        "# Monetization / Price complaints\n",
        "MONETIZATION_LEXICON = [\n",
        "    \"microtransaction\",\n",
        "    \"mtx\",\n",
        "    \"lootbox\",\n",
        "    \"dlc\",\n",
        "    \"pay2win\",  \n",
        "    \"pay-to-win\",    \n",
        "    \"overpriced\",\n",
        "    \"refund\",\n",
        "    \"sale\",\n",
        "    \"expensive\",\n",
        "    \"price\",\n",
        "    \"cost\",\n",
        "    \"money\",\n",
        "    \"greedy\",\n",
        "    \"greed\",\n",
        "    \"monetization\",\n",
        "    \"premium\",\n",
        "    \"skin\",\n",
        "    \"cosmetic\",\n",
        "]\n",
        "\n",
        "# Gameplay / Fun / Content\n",
        "GAMEPLAY_LEXICON = [\n",
        "    \"fun\",\n",
        "    \"gameplay\",\n",
        "    \"combat\",\n",
        "    \"balance\",\n",
        "    \"content\",\n",
        "    \"story\",\n",
        "    \"graphic\",\n",
        "    \"soundtrack\",\n",
        "    \"music\",\n",
        "    \"replay\",\n",
        "    \"grind\",\n",
        "    \"coop\",\n",
        "    \"friend\",\n",
        "    \"enjoyable\",\n",
        "    \"enjoy\",\n",
        "    \"addictive\",\n",
        "    \"satisfying\",\n",
        "    \"satisfaction\",\n",
        "    \"mechanic\",\n",
        "    \"skill\",\n",
        "    \"competitive\",\n",
        "    \"rank\",\n",
        "    \"matchmaking\",\n",
        "    \"strategy\",\n",
        "    \"tactical\",\n",
        "    \"teamwork\",\n",
        "    \"operator\",\n",
        "    \"map\",\n",
        "    \"weapon\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_lexicon_words(text, lexicon):\n",
        "    tokens = text.split()\n",
        "    count = sum(1 for token in tokens if token in lexicon)\n",
        "    return count\n",
        "\n",
        "df['quality_count'] = (df['clean_text'].apply(lambda x: count_lexicon_words(x, QUALITY_LEXICON))) / df['token_count'] * 100\n",
        "df['money_count'] = (df['clean_text'].apply(lambda x: count_lexicon_words(x, MONETIZATION_LEXICON))) / df['token_count'] * 100\n",
        "df['gameplay_count'] = (df['clean_text'].apply(lambda x: count_lexicon_words(x, GAMEPLAY_LEXICON))) / df['token_count'] * 100\n",
        "\n",
        "df['quality_count'] = df['quality_count'].fillna(0)\n",
        "df['money_count'] = df['money_count'].fillna(0)\n",
        "df['gameplay_count'] = df['gameplay_count'].fillna(0)\n",
        "\n",
        "df['exclamation_count'] = df['review'].apply(lambda x: x.count('!'))\n",
        "\n",
        "PROFANITY_LIST = ['shit', 'fuck', 'damn', 'crap', 'hell', 'ass', 'bitch']\n",
        "df['profanity_count'] = df['clean_text'].apply(lambda x: sum(1 for word in PROFANITY_LIST if word in x.split()))\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorize lemmatized text for topic modeling\n",
        "vectorizer = CountVectorizer(\n",
        "    min_df=5,\n",
        "    max_df=0.8, \n",
        "    max_features=5000, \n",
        "    lowercase=False,    \n",
        "    stop_words='english',\n",
        "    token_pattern=r'\\b[a-z]+\\b'\n",
        ")\n",
        "\n",
        "X_counts = vectorizer.fit_transform(df['clean_text'])\n",
        "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out()):,}\")\n",
        "print(f\"Document-term matrix shape: {X_counts.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_TOPICS = 5\n",
        "\n",
        "print(f\"\\nFitting LDA model with {N_TOPICS} topics...\")\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=N_TOPICS,\n",
        "    random_state=42,\n",
        "    max_iter=20,\n",
        "    learning_method='batch'\n",
        ")\n",
        "\n",
        "topic_model = lda.fit(X_counts)\n",
        "print(\"Topic model fitted\")\n",
        "\n",
        "topic_proportions = lda.transform(X_counts)\n",
        "print(f\"Topic proportions shape: {topic_proportions.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add topic proportion features to dataframe\n",
        "for i in range(N_TOPICS):\n",
        "    df[f'topic_{i}'] = topic_proportions[:, i]\n",
        "\n",
        "print(f\"Added {N_TOPICS} topic proportion features\")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "print(f\"\\nTop 10 words for each of {N_TOPICS} topics:\")\n",
        "print(\"=\" * 80)\n",
        "for topic_idx in range(N_TOPICS):\n",
        "    top_words_idx = topic_model.components_[topic_idx].argsort()[-10:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    top_weights = [topic_model.components_[topic_idx][i] for i in top_words_idx]\n",
        "    \n",
        "    print(f\"\\nTopic {topic_idx}:\")\n",
        "    for word, weight in zip(top_words, top_weights):\n",
        "        print(f\"  {word:15s} {weight:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nFinal dataset shape: {df.shape}\")\n",
        "feature_cols = ['quality_count', 'money_count', 'gamemplay_count', 'token_count', 'exclamation_count', 'profanity_count']\n",
        "topic_cols = [f'topic_{i}' for i in range(N_TOPICS)]\n",
        "all_features = feature_cols + topic_cols\n",
        "\n",
        "print(f\"Total number of features: {len(all_features)}\")\n",
        "print(f\"Total features:{all_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Descriptive Analysis\n",
        "\n",
        "We'll compare lexicon features and topic proportions between negative reviews that still recommend the game and those that do not. This gives an initial, lecture-style view of whether gameplay, bugs/performance, and monetization talk differ across the two groups before fitting a formal regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Group sizes (negative reviews only):\")\n",
        "print(df['recommended'].value_counts().rename(index={0: 'Not recommended', 1: 'Recommended'}))\n",
        "\n",
        "lex_cols = ['quality_count', 'money_count', 'gameplay_count']\n",
        "summary_lex = df.groupby('recommended')[lex_cols].agg(['mean', 'std', 'median'])\n",
        "summary_lex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# t-test and p-value significance results\n",
        "rows = []\n",
        "for col in lex_cols:\n",
        "    g_rec = df.loc[df['recommended'] == 1, col]\n",
        "    g_not = df.loc[df['recommended'] == 0, col]\n",
        "    t_stat, p_val = stats.ttest_ind(g_rec, g_not, equal_var=False, nan_policy='omit')\n",
        "    rows.append({\n",
        "        'feature': col,\n",
        "        'mean_rec': g_rec.mean(),\n",
        "        'mean_not': g_not.mean(),\n",
        "        't_stat': t_stat,\n",
        "        'p_value': p_val,\n",
        "    })\n",
        "\n",
        "lex_test_results = pd.DataFrame(rows)\n",
        "print(\"\\nT-test results for lexicon features:\")\n",
        "lex_test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# graph for t-test and p-value significance results\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-whitegrid')\n",
        "    except:\n",
        "        plt.style.use('seaborn')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "plt.rcParams['mathtext.default'] = 'regular'\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "lex_test_results['feature_label'] = lex_test_results['feature'].str.replace('_count', '').str.replace('_', ' ').str.title()\n",
        "lex_test_results['feature_label'] = lex_test_results['feature_label'].replace({\n",
        "    'Quality': 'Quality/Bug',\n",
        "    'Money': 'Monetization',\n",
        "    'Gameplay': 'Gameplay'\n",
        "})\n",
        "\n",
        "lex_test_results_sorted = lex_test_results.copy()\n",
        "lex_test_results_sorted['is_positive'] = lex_test_results_sorted['t_stat'] > 0\n",
        "lex_test_results_sorted['abs_t_stat'] = lex_test_results_sorted['t_stat'].abs()\n",
        "lex_test_results_sorted = lex_test_results_sorted.sort_values(['is_positive', 'abs_t_stat'], ascending=[False, False])\n",
        "lex_test_results_sorted = lex_test_results_sorted.drop(['is_positive', 'abs_t_stat'], axis=1)\n",
        "\n",
        "colors = ['#2E86AB' if x < 0 else '#A23B72' for x in lex_test_results_sorted['t_stat']]\n",
        "bars = ax1.barh(lex_test_results_sorted['feature_label'], lex_test_results_sorted['t_stat'], \n",
        "                color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "for i, (idx, row) in enumerate(lex_test_results_sorted.iterrows()):\n",
        "    p_val = row['p_value']\n",
        "    t_stat = row['t_stat']\n",
        "    \n",
        "    if p_val < 0.001:\n",
        "        sig_text = '***'\n",
        "    elif p_val < 0.01:\n",
        "        sig_text = '**'\n",
        "    elif p_val < 0.05:\n",
        "        sig_text = '*'\n",
        "    else:\n",
        "        sig_text = ''\n",
        "    \n",
        "    x_pos = t_stat + (0.5 if t_stat > 0 else -0.5)\n",
        "    ax1.text(x_pos, i, sig_text, ha='center' if t_stat > 0 else 'center', \n",
        "             va='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "ax1.axvline(x=0, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
        "\n",
        "ax1.set_xlabel('T-statistic', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('T-test Statistics by Feature\\n(Recommended vs. Not Recommended)', fontsize=13, fontweight='bold', pad=15)\n",
        "ax1.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "ax1.spines['top'].set_visible(False)\n",
        "ax1.spines['right'].set_visible(False)\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2E86AB', alpha=0.8, label='Lower in Recommended'),\n",
        "    Patch(facecolor='#A23B72', alpha=0.8, label='Higher in Recommended'),\n",
        "    plt.Line2D([0], [0], marker='', linestyle='', label='*** p < 0.001')\n",
        "]\n",
        "ax1.legend(handles=legend_elements, loc='upper right', fontsize=9, framealpha=0.9)\n",
        "\n",
        "lex_test_results_sorted['neg_log10_p'] = -np.log10(lex_test_results_sorted['p_value'])\n",
        "colors2 = ['#2E86AB' if x < 0 else '#A23B72' for x in lex_test_results_sorted['t_stat']]\n",
        "\n",
        "bars2 = ax2.barh(lex_test_results_sorted['feature_label'], lex_test_results_sorted['neg_log10_p'],\n",
        "                 color=colors2, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "ax2.axvline(x=-np.log10(0.05), color='red', linestyle='--', linewidth=1, alpha=0.7, label='p = 0.05')\n",
        "ax2.axvline(x=-np.log10(0.01), color='orange', linestyle='--', linewidth=1, alpha=0.7, label='p = 0.01')\n",
        "ax2.axvline(x=-np.log10(0.001), color='green', linestyle='--', linewidth=1, alpha=0.7, label='p = 0.001')\n",
        "\n",
        "ax2.set_xlabel(r'$-log_{10}$(p-value)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('')\n",
        "ax2.set_title('Statistical Significance\\n' + r'($-log_{10}$ p-values)', fontsize=13, fontweight='bold', pad=15)\n",
        "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "ax2.spines['top'].set_visible(False)\n",
        "ax2.spines['right'].set_visible(False)\n",
        "ax2.legend(loc='lower right', fontsize=9, framealpha=0.9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('t_test_results.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# identify which topics are most different between recommended and not recommended reviews\n",
        "\n",
        "topic_cols = [c for c in df.columns if c.startswith('topic_')]\n",
        "\n",
        "topic_means = df.groupby('recommended')[topic_cols].mean().T\n",
        "topic_means.columns = ['mean_not_recommended', 'mean_recommended']\n",
        "topic_means['diff_rec_minus_not'] = topic_means['mean_recommended'] - topic_means['mean_not_recommended']\n",
        "topic_means_sorted = topic_means.reindex(topic_means['diff_rec_minus_not'].abs().sort_values(ascending=False).index)\n",
        "display(topic_means_sorted.head(10))\n",
        "\n",
        "top_topics_for_model = list(topic_means_sorted.head(5).index)\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "plt.rcParams['mathtext.default'] = 'regular'\n",
        "\n",
        "topic_viz = topic_means_sorted.head(10).copy()\n",
        "topic_viz = topic_viz.sort_values('diff_rec_minus_not', ascending=True)  # Sort for better visualization\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "colors = ['#2E86AB' if x < 0 else '#A23B72' for x in topic_viz['diff_rec_minus_not']]\n",
        "\n",
        "bars = ax.barh(topic_viz.index, topic_viz['diff_rec_minus_not'], \n",
        "                color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Difference in Topic Proportion\\n(Recommended - Not Recommended)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Topic', fontsize=12, fontweight='bold')\n",
        "ax.set_title('Topic Differences Between Recommended and Not-Recommended Reviews', fontsize=13, fontweight='bold', pad=15)\n",
        "ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "legend_elements = [\n",
        "    Patch(facecolor='#2E86AB', alpha=0.8, label='More common in Not Recommended'),\n",
        "    Patch(facecolor='#A23B72', alpha=0.8, label='More common in Recommended')\n",
        "]\n",
        "\n",
        "ax.legend(handles=legend_elements, loc='lower right', fontsize=9, framealpha=0.9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('topic_differences.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Logistic Regression for Hypothesis Testing\n",
        "\n",
        "Next, we fit a logistic regression model predicting whether a negative review still recommends the game from lexicon features and a small set of the most distinctive topics. This mirrors the lecture pattern of using interpretable coefficients to test whether gameplay vs bug/monetization talk helps explain recommendation decisions among negative reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# logistic regression\n",
        "\n",
        "lex_cols = ['quality_count', 'money_count', 'gameplay_count']\n",
        "base_features = lex_cols + ['token_count']\n",
        "\n",
        "topic_cols = [c for c in df.columns if c.startswith('topic_')]\n",
        "selected_topics = [t for t in top_topics_for_model if t in topic_cols]\n",
        "\n",
        "feature_cols = base_features + selected_topics\n",
        "print(\"Features used in logistic regression:\")\n",
        "print(feature_cols)\n",
        "\n",
        "model_df = df.dropna(subset=feature_cols + ['recommended']).copy()\n",
        "\n",
        "X = model_df[feature_cols]\n",
        "y = model_df['recommended']\n",
        "\n",
        "log_reg = LogisticRegression(max_iter=1000)\n",
        "log_reg.fit(X, y)\n",
        "\n",
        "coef_series = pd.Series(log_reg.coef_[0], index=feature_cols)\n",
        "odds_ratios = np.exp(coef_series)\n",
        "odds_ratios_df = pd.DataFrame({\n",
        "    'coef': coef_series,\n",
        "    'odds_ratio': odds_ratios\n",
        "})\n",
        "\n",
        "print(\"\\nOdds ratios for lexicon predictors:\")\n",
        "print(odds_ratios_df.loc[lex_cols, :])\n",
        "\n",
        "print(\"\\nOdds ratios for selected topic predictors:\")\n",
        "print(odds_ratios_df.loc[selected_topics, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Automated Robustness Checks\n",
        "\n",
        "We systematically test the sensitivity of our findings to different parameter choices by varying:\n",
        "- **Sentiment threshold (`NEG_THRESHOLD`)**: Different cutoffs for what counts as \"negative\"\n",
        "- **Number of topics (`N_TOPICS`)**: Different numbers of topics in the LDA model\n",
        "\n",
        "The automated pipeline runs the full analysis for each parameter combination and compares the results.\n",
        "\n",
        "**Note:** This pipeline requires data with `sentiment_compound` and `clean_text` columns from all reviews (before filtering to negative only). If running this after the main analysis, make sure to save `df_full` after Step 2 (sentiment analysis) but before Step 3 (filtering to negative reviews)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Interpretation & Summary\n",
        "\n",
        "**Key findings:** Among negative-sentiment Rainbow Six Siege reviews, those that still recommend the game use **more gameplay/fun/content language** and **less bug/performance and monetization language** than those that do not recommend the game. In the logistic regression, `bug_lex_per_100` and `monet_lex_per_100` have **negative odds ratios** (more complaints about bugs or money make a negative review less likely to be a recommendation), while `gameplay_lex_per_100` has a **positive odds ratio** (more gameplay talk makes a negative review more likely to be a recommendation).\n",
        "\n",
        "**Hypothesis check:** This pattern is exactly what your hypothesis predicted: \"negative but recommended\" reviews emphasize core gameplay enjoyment, while \"negative non-recommended\" reviews focus more on technical and monetization problems. The magnitudes of the odds ratios are modest but consistent, which is reasonable given the noisy, informal review text.\n",
        "\n",
        "**Limitations and caveats:** Results rely on VADER for sentiment (lexicon-based and not game-specific), simple hand-crafted lexicons, and a bag-of-words LDA topic model; more advanced embeddings or transformer-based sentiment could refine the analysis but are beyond the scope of the course. There may also be selection bias in who writes Steam reviews, and we only analyze one game, so the findings should be interpreted as evidence consistent with the hypothesis for Rainbow Six Siege rather than a universal law of all games."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3350",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
